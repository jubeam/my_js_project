{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class_WNR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jubeam/my_js_project/blob/master/Class_WNR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv_wgizoajiB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c030c5a0-ba95-438a-ce09-e27223131021"
      },
      "source": [
        "#0. 사용할 패키지 불러오기\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 랜덤시드 고정시키기\n",
        "np.random.seed(3)\n",
        "\n",
        "# 1. 데이터 생성하기\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,         # 픽셀 값을 0~1 범위로 변환\n",
        "        rotation_range=15,      # 15도까지 회전\n",
        "        width_shift_range=0.1,  # 10%까지 좌우 이동\n",
        "        height_shift_range=0.1, # 10%까지 상하 이동\n",
        "        shear_range=0.5,        # 50%까지 기울임\n",
        "        zoom_range=[0.8,2.0],         # 20%까지 확대\n",
        "        horizontal_flip=True,   # 좌우 뒤집기\n",
        "        vertical_flip=True,\n",
        "       fill_mode='nearest')\n",
        "    \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/gdrive/My Drive/Colab Notebooks/WNR/train',\n",
        "        target_size=(70, 70),\n",
        "        batch_size=3,\n",
        "        class_mode='categorical')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        '/gdrive/My Drive/Colab Notebooks/WNR/test',\n",
        "        target_size=(70, 70),    \n",
        "        batch_size=3,\n",
        "       class_mode='categorical')\n",
        "real_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "real_test_generator = real_test_datagen.flow_from_directory(\n",
        "        '/gdrive/My Drive/Colab Notebooks/WNR/real_test',\n",
        "        target_size=(70, 70),    \n",
        "        batch_size=3,\n",
        "       class_mode='categorical')\n",
        "\n",
        "# 2. 모델 구성하기\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(7, 7),\n",
        "                 activation='relu',\n",
        "                 input_shape=(70,70,3)))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# 3. 모델 학습과정 설정하기\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 4. 모델 학습시키기\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(patience = 10) # 조기종료 콜백함수 정의\n",
        "#hist = model.fit(X_train, Y_train, epochs=3000, batch_size=10, validation_data=(X_val, Y_val), callbacks=[early_stopping])\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=15 * 100,\n",
        "        epochs=100,\n",
        "        validation_data=test_generator,\n",
        "        validation_steps=5,\n",
        "        callbacks=[early_stopping]    \n",
        "        )\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "print(\"-- Evaluate --\")\n",
        "scores = model.evaluate_generator(test_generator, steps=5)\n",
        "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# 7. 모델 사용하기\n",
        "print(\"-- Predict --\")\n",
        "output = model.predict_generator(real_test_generator, steps=5)\n",
        "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
        "print(real_test_generator.class_indices)\n",
        "print(output)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "Found 70 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0830 19:44:58.162312 140040561047424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0830 19:44:58.211556 140040561047424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0830 19:44:58.220382 140040561047424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0830 19:44:58.279253 140040561047424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 2 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0830 19:44:58.320360 140040561047424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0830 19:44:58.343412 140040561047424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0830 19:44:58.482297 140040561047424 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0830 19:44:58.541936 140040561047424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1500/1500 [==============================] - 217s 145ms/step - loss: 0.5329 - acc: 0.7133 - val_loss: 0.1743 - val_acc: 0.7692\n",
            "Epoch 2/100\n",
            "1500/1500 [==============================] - 200s 133ms/step - loss: 0.3231 - acc: 0.8669 - val_loss: 0.1823 - val_acc: 0.8462\n",
            "Epoch 3/100\n",
            "1500/1500 [==============================] - 198s 132ms/step - loss: 0.2814 - acc: 0.8833 - val_loss: 0.2487 - val_acc: 0.7692\n",
            "Epoch 4/100\n",
            "1500/1500 [==============================] - 197s 132ms/step - loss: 0.2428 - acc: 0.9002 - val_loss: 0.3091 - val_acc: 0.6364\n",
            "Epoch 5/100\n",
            "1500/1500 [==============================] - 199s 133ms/step - loss: 0.2105 - acc: 0.9193 - val_loss: 0.0757 - val_acc: 1.0000\n",
            "Epoch 6/100\n",
            "1500/1500 [==============================] - 200s 134ms/step - loss: 0.2033 - acc: 0.9213 - val_loss: 0.0610 - val_acc: 1.0000\n",
            "Epoch 7/100\n",
            "1500/1500 [==============================] - 198s 132ms/step - loss: 0.2245 - acc: 0.9187 - val_loss: 0.0427 - val_acc: 1.0000\n",
            "Epoch 8/100\n",
            "1500/1500 [==============================] - 195s 130ms/step - loss: 0.1968 - acc: 0.9231 - val_loss: 0.0573 - val_acc: 1.0000\n",
            "Epoch 9/100\n",
            "1500/1500 [==============================] - 196s 130ms/step - loss: 0.1942 - acc: 0.9240 - val_loss: 0.0031 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            "1500/1500 [==============================] - 196s 131ms/step - loss: 0.1795 - acc: 0.9320 - val_loss: 0.0103 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            "1500/1500 [==============================] - 198s 132ms/step - loss: 0.1742 - acc: 0.9322 - val_loss: 0.0623 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            "1500/1500 [==============================] - 210s 140ms/step - loss: 0.1613 - acc: 0.9353 - val_loss: 0.1229 - val_acc: 0.9091\n",
            "Epoch 13/100\n",
            "1500/1500 [==============================] - 232s 155ms/step - loss: 0.1690 - acc: 0.9356 - val_loss: 0.0396 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "1500/1500 [==============================] - 255s 170ms/step - loss: 0.1444 - acc: 0.9433 - val_loss: 0.2082 - val_acc: 0.9231\n",
            "Epoch 15/100\n",
            "1500/1500 [==============================] - 235s 156ms/step - loss: 0.1500 - acc: 0.9418 - val_loss: 0.0523 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "1500/1500 [==============================] - 244s 163ms/step - loss: 0.1518 - acc: 0.9416 - val_loss: 0.0096 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "1500/1500 [==============================] - 237s 158ms/step - loss: 0.1343 - acc: 0.9471 - val_loss: 0.0859 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "1500/1500 [==============================] - 246s 164ms/step - loss: 0.1341 - acc: 0.9478 - val_loss: 0.0192 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "1500/1500 [==============================] - 235s 157ms/step - loss: 0.1296 - acc: 0.9482 - val_loss: 0.0335 - val_acc: 1.0000\n",
            "-- Evaluate --\n",
            "acc: 100.00%\n",
            "-- Predict --\n",
            "{'cancer': 0, 'normal': 1}\n",
            "[[1.000 0.000]\n",
            " [0.017 0.983]\n",
            " [1.000 0.000]\n",
            " [0.017 0.983]\n",
            " [0.017 0.983]\n",
            " [1.000 0.000]\n",
            " [1.000 0.000]\n",
            " [0.017 0.983]\n",
            " [1.000 0.000]\n",
            " [0.017 0.983]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}